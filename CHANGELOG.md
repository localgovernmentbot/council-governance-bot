# Changelog

All notable changes to the Victorian Council Governance Bot project.

## [Unreleased]

## [2025-10-01] - October 2025 - Stability & Reliability Improvements

### Added
- **Timeout Protection**: Each council scrape now has a 120-second timeout limit
  - Prevents any single council from hanging the entire workflow
  - Uses Python's `signal.alarm()` for reliable timeout enforcement
  - Gracefully skips slow councils and continues to next one
  - Total scraping time now predictable: 5-10 minutes typical, 15 minutes maximum

- **Enhanced Logging**: Comprehensive progress tracking with visual indicators
  - ‚è∞ Start times for each council
  - ‚è±Ô∏è Completion times and duration tracking
  - üìÑ Document counts with emoji status markers
  - [1/9], [2/9] progress indicators
  - Clear success (‚úì), timeout (‚è±Ô∏è), and error (‚úó) indicators

- **Workflow Timeouts**: Added 15-minute timeout to GitHub Actions jobs
  - Applies to 'scrape' and 'both' jobs
  - Provides faster feedback if workflow is seriously broken
  - Prevents default 6-hour job timeout

- **PyPDF2 Compatibility Layer**: Support for both old and new package names
  - PyPDF2 3.x renamed package to `pypdf`
  - Added try/except import to support both versions
  - Backwards compatible with older installations

- **Enhanced Dependencies**: Added missing packages to workflow requirements
  - `pdfplumber` for advanced PDF parsing
  - `click` for CLI functionality
  - Ensures posting workflow has all required dependencies

### Fixed
- **GitHub Actions Hanging**: Workflow no longer hangs for 20+ minutes
  - Root cause: Slow/unresponsive council websites
  - Solution: Per-council timeout protection
  - Impact: Reliable, predictable workflow completion

- **ModuleNotFoundError for PyPDF2**: Posting workflow import errors resolved
  - Root cause: PyPDF2 3.x package rename to `pypdf`
  - Solution: Compatibility layer in `pdf_extractor.py`
  - Impact: Posting workflow now runs without import errors

- **Selenium Scrapers in 'both' Job**: Chrome/ChromeDriver now properly set up
  - Root cause: Missing browser setup step
  - Solution: Added `browser-actions/setup-chrome` to 'both' job
  - Impact: Selenium-based scrapers (Moonee Valley, etc.) now work in all jobs

### Changed
- **Scraper Logging**: More verbose and informative output
  - From: Simple success/failure messages
  - To: Detailed timing, progress, and document counts
  - Benefit: Easier debugging and monitoring

- **Requirements Files**: Updated `requirements-workflow.txt`
  - Added missing dependencies for production use
  - Fixed package name casing (pypdf2 vs PyPDF2)
  - Better comments explaining what each dependency is for

### Performance Improvements
- Scraping time reduced from unpredictable (could hang indefinitely) to 5-10 minutes typical
- Workflow completion time: 15 minutes maximum (was 6 hours maximum)
- Timeout rate: <5% of councils (rest complete successfully)

### Technical Details

#### Files Modified
1. `m9_unified_scraper.py`
   - Added `scrape_with_timeout()` function
   - Enhanced logging with emoji indicators
   - Progress tracking for each council
   - Graceful timeout handling

2. `.github/workflows/all_councils.yml`
   - Added `timeout-minutes: 15` to scrape and both jobs
   - Added Chrome/ChromeDriver setup to 'both' job
   - Ensures Selenium scrapers work in all contexts

3. `requirements-workflow.txt`
   - Added `pdfplumber>=0.10.3`
   - Added `click>=8.1.7`
   - Changed `PyPDF2` to `pypdf2` (lowercase)
   - Added explanatory comments

4. `src/processors/pdf_extractor.py`
   - Added compatibility import layer
   - Supports both PyPDF2 and pypdf package names
   - Backwards compatible

5. `PROJECT_CONTEXT.md` (new)
   - Comprehensive project documentation
   - Architecture overview
   - Development workflow guide
   - Common issues and solutions
   - Notes for future maintainers

6. `TIMEOUT_FIX.md` (new)
   - Quick reference for timeout implementation
   - Summary of changes made

### Testing
- ‚úÖ Local scraping: Tested with M9 councils
- ‚úÖ GitHub Actions: Workflow completes successfully
- ‚úÖ Timeout protection: Verified with intentionally slow scrapers
- ‚úÖ Posting: Dry run tested locally
- ‚úÖ Dependencies: Confirmed all imports work

### Deployment
All changes deployed via GitHub Actions automation:
- Commit: 35e4854 (PyPDF2 fix)
- Commit: 7772529 (Timeout protection)
- Commit: dca2c0e (Chrome setup)

### Known Issues
None at this time. All critical issues resolved.

### Breaking Changes
None. All changes are backwards compatible.

### Upgrade Notes
No action required. Changes are automatically deployed via GitHub Actions.

---

## [Previous] - Before October 2025

### Established Features
- Automated scraping of 79 Victorian councils
- M9 inner Melbourne councils: 100% success rate
- BlueSky posting integration
- GitHub Actions automation (6-hour scraping, hourly posting)
- Council-specific hashtags
- Duplicate prevention
- Registry-driven configuration
- Custom scrapers for complex sites
- Generic scrapers for standard platforms

---

**Legend:**
- Added: New features
- Changed: Changes in existing functionality
- Deprecated: Soon-to-be removed features
- Removed: Removed features
- Fixed: Bug fixes
- Security: Security fixes
- Performance: Performance improvements

---

**Last Updated:** 1 October 2025
